# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: Взял за основу 2 метрики:
1. Количество операций в секунду на небольщом объёме данных (18 строк)
2. Время выполения на объёме данных в 20к строк

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений и времени выполения скрипта в 30 секунд

Вот как я построил `feedback_loop`: Написал тесты на производительность для разного объма данных:
1. 1й тест проверял количество выполняемых операций в секунду
2. 2й тест проверял среднее время прохождения теста на выборке в 20к строк.

После каждого нахождения и исправления точек роста, правились  тесты для недопущения регресси в результате рефекторинга.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался: `stackprof`, `ruby_callgrind`, `ruby_prof_graf` - для нахождения точек роста, а так же частоты вызова тех или иных методов, `htop` и `ruby_spy` - для отслеживания процесса, просмотра информации о потребляемой памяти и на каком этапе выполения находится скрипт

Вот какие проблемы удалось найти и решить

### Находка №1
Поиск сессии вызывается на каждого пользователя и проходит по всем сессиям каждый, затратная операция:
```
user_sessions = sessions.select { |session| session['user_id'] == user['id'] }
```

Решил данную проблему сразу сгруппировав сессию по user_id
На выборке в 10_000 строк, скорость работы выросла с минимальных 3.3 сек до 0.335, выигрыш составил 10х.

### Находка №2
Далее по коду проблемным стал collect_stats_from_users метод, не сама реализация, а количество неоправданных вызовов

Выигрыш в производительности на выборке в 20к составил 0.015с.

### Находка №3
Далее метод
```
sessions.each do |session|
    browser = session['browser']
    uniqueBrowsers += [browser] if uniqueBrowsers.all? { |b| b != browser }
end
```
а именно метод `uniqueBrowsers.all, который вызывается 10к раз на выборке в 10к`
Выигрыш в производительности на выборке в 20к составил 0.018с.

### Находка №4
`Date.parse` был заменён на `Date.strptime(session['date'], '%F')`, казалось бы небольшая функция, на выходе сэкономила 0.050c времени на выборке в 20к и время составило 0.8 сек

### Находка №5
Замена вызова блока через `block.call` на `yield` дало прирост ещё 0.030 и составило 0.770 сек

### Находка №6
Количество вызовов метода `upcase`, преобразование вызывалось на одних и тех же данных c дополнительной итерацией, в результате удалось сократить время с 0.770 до 0.75

### Находка №7
В самом 1м блоке `each` было найдено дублирование метода `split` . Удалось сократить время с 0.75 до 0.731

### Находка №8
Данные для `report['allBrowsers']` брались путём дополнительной итерация, удалил дополнительную итерацию, использовал данные из `uniqueBrowsers`, сократил время c 0.731 до 0.716

### Находка №9
Перебрал схему, понял, что входные данные приходят в определённом порядке, можно на этом сыграть и проводить меньше итерация и довести до линейной зависимости. В результате сократил время работы с 0.716 до 0.136

### Находка №10
Бесполезный Date.strftime, заменён на сhomp. Заменён each + push, на map. `to_json` -медленный, заменён на `oj`.В результате сокращено время работы с 0.136 до 0.064

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с бесконечно долго выполнения до 33 секунд

Было весело :)

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написаны 2 теста производительности.
